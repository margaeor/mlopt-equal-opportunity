{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2e0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-09-05\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"std\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP, Gurobi, Random, Statistics, Combinatorics, LinearAlgebra\n",
    "using DataFrames, CSV, IterTools\n",
    "using Random\n",
    "using GLMNet, StatsBase\n",
    "\n",
    "seed = 2\n",
    "gurobi_env = Gurobi.Env()\n",
    "Random.seed!(seed)\n",
    "\n",
    "df_path = \"data/output/preprocessed.csv\"\n",
    "predictor_col = \"income_total\"\n",
    "normalization_type = \"std\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dd0b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparse_regression (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_r2(X, y, beta)\n",
    "    X = augment_X(X)\n",
    "    SSres = sum( (y .- X*beta).^2 )\n",
    "    SStot = sum( (y .- Statistics.mean(y)).^2 )\n",
    "    return 1-SSres/SStot\n",
    "end\n",
    "\n",
    "function grid_search(X, y, solver_func, error_func, error_strategy=\"Min\",train_val_ratio=0.7;params... )\n",
    "\n",
    "    # Split the data into training/validation\n",
    "    X_train, y_train, X_val, y_val = partitionTrainTest(X, y, train_val_ratio);\n",
    "    \n",
    "    # Create the grid (i.e. all the combinations of the given parameters)\n",
    "    param_names = keys(params)\n",
    "    param_combinations = [\n",
    "        Dict(param_names[i]=>p[i] for i in 1:length(param_names)) \n",
    "        for p in product([params[i] for i in keys(params)]...)\n",
    "    ]\n",
    "    \n",
    "    # Initialize variables used to hold validation information\n",
    "    error_multiplier = error_strategy == \"Min\" ? 1 : -1\n",
    "    best_error = Inf # We consider minimization\n",
    "    best_param_set = []\n",
    "    \n",
    "    # Iterate over all combinations of parameters\n",
    "    for param_comb in param_combinations\n",
    "        \n",
    "        # Optimize model and find optimal variables\n",
    "        model_vars = solver_func(X_train,y_train;param_comb...)\n",
    "        \n",
    "        # Evaluate model error on validation set\n",
    "        if model_vars isa Tuple\n",
    "            error = error_multiplier*error_func(X_val, y_val, model_vars...)\n",
    "        else\n",
    "            error = error_multiplier*error_func(X_val, y_val, model_vars)\n",
    "        end\n",
    "        \n",
    "        # If error is better than the best error so far, keep track \n",
    "        # of the error and the params\n",
    "        if error < best_error\n",
    "            best_error = error \n",
    "            best_param_set = param_comb\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Retrain the model on the whole training set \n",
    "    # using the best set of params\n",
    "    model_vars = solver_func(X,y;best_param_set...)\n",
    "    \n",
    "    # Return the model variable and the best params\n",
    "    return model_vars, best_param_set\n",
    "end\n",
    "\n",
    "function normalize_data(X, method=\"minmax\"; is_train=true)\n",
    "    X = copy(X)\n",
    "    if is_train\n",
    "        global nonzero_idx = findall([maximum(X[:,i])-minimum(X[:,i]) for i = 1:size(X,2)].>=0.01)\n",
    "        if method == \"std\"\n",
    "            global dt=fit(ZScoreTransform, X[:,nonzero_idx]; dims=1, center=true, scale=true)\n",
    "        elseif method == \"minmax\"\n",
    "            global dt=fit(UnitRangeTransform, X[:,nonzero_idx]; dims=1, unit=true)\n",
    "        end\n",
    "    end\n",
    "    X[:,nonzero_idx] = StatsBase.transform(dt, X[:,nonzero_idx])\n",
    "    \n",
    "    return X\n",
    "end\n",
    "\n",
    "\n",
    "function partitionTrainTest(X,y, at = 0.7, s=seed)\n",
    "    n = size(X,1)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    return X[train_idx,:], y[train_idx], X[test_idx,:], y[test_idx]\n",
    "end\n",
    "\n",
    "function augment_X(X)\n",
    "    return [X ones(size(X,1),1)]\n",
    "end\n",
    "\n",
    "function solve_holistic_regr(X,y;gamma,rho,k)\n",
    "    C = cor(X)\n",
    "    n,p = size(X)\n",
    "    X_aug = augment_X(X)\n",
    "    M = 10^5\n",
    "    m = Model(with_optimizer(Gurobi.Optimizer, gurobi_env))\n",
    "    set_optimizer_attribute(m, \"OutputFlag\", 0)\n",
    "    set_optimizer_attribute(m, \"PSDTol\", 1)\n",
    "    @variable(m, beta[1:(p+1)])\n",
    "    @variable(m, z[1:p],Bin)\n",
    "    @variable(m, t[1:p])\n",
    "    @objective(m, Min, 1/2*sum((X_aug*beta.-y).^2)+gamma*sum(t[i] for i=1:p))\n",
    "    @constraint(m, [i=1:p], t[i]>= beta[i])\n",
    "    @constraint(m, [i=1:p], t[i]>= -beta[i])\n",
    "    @constraint(m, [i=1:p], beta[i]<= M*z[i])\n",
    "    @constraint(m, [i=1:p], -M*z[i]<=beta[i])\n",
    "    @constraint(m, sum(z)<=k)\n",
    "    #@constraint(m, [i=1:4:p-3], sum(z[i+j] for j=0:3)<=1)\n",
    "#     for i in 1:p\n",
    "#         for j in i+1:p\n",
    "#             if abs(C[i,j]) > rho\n",
    "#                 @constraint(m, z[i]+z[j] <= 1)\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "    optimize!(m)\n",
    "    return JuMP.value.(beta)\n",
    "end\n",
    "\n",
    "function fit_lasso(X, y)\n",
    "    cv = glmnetcv(X, y);\n",
    "    id_best = argmin(cv.meanloss);\n",
    "    betas = [GLMNet.coef(cv);cv.path.a0[id_best]];\n",
    "    return betas\n",
    "end\n",
    "\n",
    "function solve_inner_problem(X,Y,s,γ)\n",
    "    indices = findall(s .> 0.5)\n",
    "    n = length(Y)\n",
    "    denom = 2*n\n",
    "    Xs = X[:, indices]\n",
    "    α = Y - Xs * (inv(I / γ + Xs' * Xs) * (Xs'* Y))\n",
    "    obj = dot(Y, α) / denom\n",
    "    tmp = X' * α\n",
    "    grad = -γ .* tmp .^ 2 ./ denom\n",
    "  return obj, grad\n",
    "end\n",
    "\n",
    "function sparse_regression(X,Y,k,γ,s0=[])\n",
    "    \n",
    "    m = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(m, \"OutputFlag\", 0)\n",
    "    n,p = size(X)\n",
    "    \n",
    "    ###\n",
    "    # Step 1: Define the Variables:\n",
    "    ###\n",
    "    @variable(m, s[1:p], Bin)\n",
    "    @variable(m, t >= 0)\n",
    "    \n",
    "    ###\n",
    "    # Step 2: Set Up Constraints and Objective\n",
    "    ###\n",
    "    @constraint(m, sum(s) <= k)\n",
    "    # Initial solution: if none is provided, start at arbitrary point\n",
    "    if length(s0) == 0\n",
    "        s0 = zeros(p)\n",
    "        s0[1:k] .= 1\n",
    "    end\n",
    "    obj0, grad0 = solve_inner_problem(X,Y, s0, γ)\n",
    "    @constraint(m, t >= obj0 + dot(grad0, s - s0))\n",
    "    # Objective\n",
    "    @objective(m, Min, t)\n",
    "    \n",
    "    ###\n",
    "    # Step 3: Define the outer approximation function\n",
    "    ###\n",
    "    function outer_approximation(cb_data)\n",
    "        s_val = []\n",
    "        for i = 1:p\n",
    "            s_val = [s_val;callback_value(cb_data, s[i])]\n",
    "        end\n",
    "        obj, grad = solve_inner_problem(X,Y, s_val, γ)\n",
    "        # add the cut: t >= obj + sum(∇s * (s - s_val))\n",
    "        offset = sum(grad .* s_val)\n",
    "        con = @build_constraint(t >= obj + sum(grad[j] * s[j] for j=1:p) - offset)    \n",
    "        MOI.submit(m, MOI.LazyConstraint(cb_data), con)\n",
    "    end\n",
    "    MOI.set(m, MOI.LazyConstraintCallback(), outer_approximation)\n",
    "\n",
    "    ###\n",
    "    # Step 4: Solve\n",
    "    ###\n",
    "    optimize!(m)\n",
    "    s_opt = JuMP.value.(s)\n",
    "    s_nonzeros = findall(x -> x>0.5, s_opt)\n",
    "    β = zeros(p)\n",
    "    X_s = X[:, s_nonzeros]\n",
    "    # Formula for the nonzero coefficients\n",
    "    β[s_nonzeros] = γ * X_s' * (Y - X_s * ((I / γ + X_s' * X_s) \\ (X_s'* Y)))\n",
    "    \n",
    "    return Dict(\"support\" => s_opt, \"coefs\" => β, \"selected_features\" => s_nonzeros)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b9d23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271-element Vector{String}:\n",
       " \"age\"\n",
       " \"cost_fuel\"\n",
       " \"cost_gas\"\n",
       " \"earnings_total\"\n",
       " \"electricity_cost\"\n",
       " \"family_people_number\"\n",
       " \"gross_rent\"\n",
       " \"gross_rent_pcnt_income\"\n",
       " \"household_people_number\"\n",
       " \"income_adjustment_factor\"\n",
       " \"income_all\"\n",
       " \"income_family\"\n",
       " \"income_household\"\n",
       " ⋮\n",
       " \"field_of_degree_54\"\n",
       " \"field_of_degree_55\"\n",
       " \"field_of_degree_56\"\n",
       " \"field_of_degree_57\"\n",
       " \"field_of_degree_59\"\n",
       " \"field_of_degree_60\"\n",
       " \"field_of_degree_61\"\n",
       " \"field_of_degree_62\"\n",
       " \"num_degrees_0.0\"\n",
       " \"num_degrees_1.0\"\n",
       " \"num_degrees_2.0\"\n",
       " \"num_degrees_nan\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(CSV.File(df_path, header=1))\n",
    "df = last(df, 1000)\n",
    "names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9da1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = [\n",
    "    \"earnings_total\",\n",
    "    \"income_interest_dividends_rental\",\n",
    "    \"income_retirement\",\n",
    "    \"income_all\",\n",
    "    \"income_social_security\",\n",
    "    \"income_supplementary_security\",\n",
    "    \"income_total\",\n",
    "    \"income_self_employment\",\n",
    "    \"income_household\",\n",
    "    \"income_to_poverty_ratio\",\n",
    "    \"income_public_assistance\",\n",
    "    \"income_family\",\n",
    "    \"income_wages_salary\",\n",
    "    \"monthly_owner_costs\",\n",
    "    \"gross_rent\",\n",
    "    \"person_number\",\n",
    "    \"rent_monthly\",\n",
    "    \"property_value\"\n",
    "]\n",
    "cols = filter(x -> x ∉ excluded_cols, names(df))\n",
    "X, y = Matrix{Float32}(df[!, filter(x -> x != predictor_col, cols)]), df[!,predictor_col]\n",
    "\n",
    "X_train, y_train, X_test, y_test = partitionTrainTest(X, y, 0.7);\n",
    "X_train = normalize_data(X_train, normalization_type; is_train=true);\n",
    "X_test = normalize_data(X_test, normalization_type; is_train=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "#betas, params = grid_search(X_train, y_train, solve_holistic_regr, calc_r2, \"Max\", 0.7; gamma=[0.1], rho=[0.7], k=[20])\n",
    "#betas = fit_lasso(X_train, y_train)\n",
    "betas = sparse_regression(X_train, y_train, k ,1/sqrt(size(X_train,1)))\n",
    "r2_c = calc_r2(X_test, y_test, betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = filter(x -> x ∉ excluded_cols, names(df))\n",
    "#important_features = cols[findall(abs.(betas_hol) .>= 0.01)]\n",
    "THRESHOLD = 0.000001\n",
    "println(\"Most important features:\")\n",
    "for i in sortperm(abs.(betas[2:end]), rev=true)\n",
    "    if abs(betas[i])<=THRESHOLD\n",
    "        continue\n",
    "    end\n",
    "    println(\"- $(cols[i]) : $(betas[i+1])\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8eb5c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000×253 Matrix{Float32}:\n",
       "  1.0738    -0.220842    0.175069    0.565204   …  -0.741598  -0.234882  0.0\n",
       "  1.51813   -0.220842   -0.625123    1.08324       -0.741598  -0.234882  0.0\n",
       " -1.37      -0.220842   -0.625123   -0.263652      -0.741598   4.2574    0.0\n",
       " -0.370263  -0.220842   -0.625123   -1.50694        1.34842   -0.234882  0.0\n",
       "  0.296228  -0.220842   -0.358392    0.668811       1.34842   -0.234882  0.0\n",
       " -0.925672   3.27259    -0.625123   -0.470866   …  -0.741598  -0.234882  0.0\n",
       "  0.851638  -0.220842    0.0417037  -0.988901      -0.741598  -0.234882  0.0\n",
       "  0.851638  -0.220842   -0.625123    0.0471687     -0.741598  -0.234882  0.0\n",
       "  0.40731   -0.220842    0.0417037  -0.470866      -0.741598  -0.234882  0.0\n",
       " -0.481345  -0.220842   -0.625123   -0.263652      -0.741598  -0.234882  0.0\n",
       "  1.40705   -0.220842   -0.625123   -0.160045   …   1.34842   -0.234882  0.0\n",
       " -0.703509  -0.220842    0.575165    0.0471687      1.34842   -0.234882  0.0\n",
       "  0.185146  -0.220842    0.708531   -0.988901       1.34842   -0.234882  0.0\n",
       "  ⋮                                             ⋱   ⋮                    \n",
       " -0.814591  -0.220842   -0.625123   -0.263652      -0.741598  -0.234882  0.0\n",
       " -1.25892   -0.220842    0.308435    0.0471687     -0.741598  -0.234882  0.0\n",
       " -0.259181  -0.220842   -0.625123    0.150776   …   1.34842   -0.234882  0.0\n",
       "  1.40705   -0.220842   -0.625123   -0.160045      -0.741598  -0.234882  0.0\n",
       " -0.259181  -0.220842    0.0417037  -0.885294      -0.741598  -0.234882  0.0\n",
       " -1.14784   -0.220842    2.17555    -0.781687      -0.741598  -0.234882  0.0\n",
       "  0.296228   0.943635   -0.625123   -0.0564383     -0.741598  -0.234882  0.0\n",
       "  0.629474  -0.220842    3.37584     0.565204   …   1.34842   -0.234882  0.0\n",
       " -1.70325   -0.220842   -0.358392   -0.781687      -0.741598  -0.234882  0.0\n",
       " -0.592427  -0.220842   -0.625123   -0.574473      -0.741598  -0.234882  0.0\n",
       "  1.40705   -0.133506    0.975262   -0.885294       1.34842   -0.234882  0.0\n",
       "  0.518392  -0.0752826  -0.625123   -0.574473      -0.741598  -0.234882  0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
