{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de2e0cfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package SubsetSelection not found in current path:\n- Run `import Pkg; Pkg.add(\"SubsetSelection\")` to install the SubsetSelection package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package SubsetSelection not found in current path:\n- Run `import Pkg; Pkg.add(\"SubsetSelection\")` to install the SubsetSelection package.\n",
      "",
      "Stacktrace:",
      " [1] require(into::Module, mod::Symbol)",
      "   @ Base .\\loading.jl:893",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "using JuMP, Gurobi, Random, Statistics, Combinatorics, LinearAlgebra\n",
    "using DataFrames, CSV, IterTools\n",
    "using Random\n",
    "using GLMNet, StatsBase\n",
    "using SubsetSelection\n",
    "\n",
    "seed = 2\n",
    "gurobi_env = Gurobi.Env()\n",
    "Random.seed!(seed)\n",
    "\n",
    "df_path = \"data/output/preprocessed.csv\"\n",
    "predictor_col = \"income_total\"\n",
    "normalization_type = \"std\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0dd0b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparse_regression (generic function with 3 methods)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calc_r2(X, y, beta)\n",
    "    X = augment_X(X)\n",
    "    SSres = sum( (y .- X*beta).^2 )\n",
    "    SStot = sum( (y .- Statistics.mean(y)).^2 )\n",
    "    return 1-SSres/SStot\n",
    "end\n",
    "\n",
    "function grid_search(X, y, solver_func, error_func, error_strategy=\"Min\",train_val_ratio=0.7;params... )\n",
    "\n",
    "    # Split the data into training/validation\n",
    "    X_train, y_train, X_val, y_val = partitionTrainTest(X, y, train_val_ratio);\n",
    "    \n",
    "    # Create the grid (i.e. all the combinations of the given parameters)\n",
    "    param_names = keys(params)\n",
    "    param_combinations = [\n",
    "        Dict(param_names[i]=>p[i] for i in 1:length(param_names)) \n",
    "        for p in product([params[i] for i in keys(params)]...)\n",
    "    ]\n",
    "    \n",
    "    # Initialize variables used to hold validation information\n",
    "    error_multiplier = error_strategy == \"Min\" ? 1 : -1\n",
    "    best_error = Inf # We consider minimization\n",
    "    best_param_set = []\n",
    "    \n",
    "    # Iterate over all combinations of parameters\n",
    "    for param_comb in param_combinations\n",
    "        \n",
    "        # Optimize model and find optimal variables\n",
    "        model_vars = solver_func(X_train,y_train;param_comb...)\n",
    "        \n",
    "        # Evaluate model error on validation set\n",
    "        if model_vars isa Tuple\n",
    "            error = error_multiplier*error_func(X_val, y_val, model_vars...)\n",
    "        else\n",
    "            error = error_multiplier*error_func(X_val, y_val, model_vars)\n",
    "        end\n",
    "        \n",
    "        # If error is better than the best error so far, keep track \n",
    "        # of the error and the params\n",
    "        if error < best_error\n",
    "            best_error = error \n",
    "            best_param_set = param_comb\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Retrain the model on the whole training set \n",
    "    # using the best set of params\n",
    "    model_vars = solver_func(X,y;best_param_set...)\n",
    "    \n",
    "    # Return the model variable and the best params\n",
    "    return model_vars, best_param_set\n",
    "end\n",
    "\n",
    "function normalize_data(X, method=\"minmax\"; is_train=true)\n",
    "    X = copy(X)\n",
    "    if is_train\n",
    "        global nonzero_idx = findall([maximum(X[:,i])-minimum(X[:,i]) for i = 1:size(X,2)].>=0.01)\n",
    "        if method == \"std\"\n",
    "            global dt=fit(ZScoreTransform, X[:,nonzero_idx]; dims=1, center=true, scale=true)\n",
    "        elseif method == \"minmax\"\n",
    "            global dt=fit(UnitRangeTransform, X[:,nonzero_idx]; dims=1, unit=true)\n",
    "        end\n",
    "    end\n",
    "    X[:,nonzero_idx] = StatsBase.transform(dt, X[:,nonzero_idx])\n",
    "    \n",
    "    return X\n",
    "end\n",
    "\n",
    "\n",
    "function partitionTrainTest(X,y, at = 0.7, s=seed)\n",
    "    n = size(X,1)\n",
    "    idx = shuffle(1:n)\n",
    "    train_idx = view(idx, 1:floor(Int, at*n))\n",
    "    test_idx = view(idx, (floor(Int, at*n)+1):n)\n",
    "    return X[train_idx,:], y[train_idx], X[test_idx,:], y[test_idx]\n",
    "end\n",
    "\n",
    "function augment_X(X)\n",
    "    return [X ones(size(X,1),1)]\n",
    "end\n",
    "\n",
    "function solve_holistic_regr(X,y;gamma,rho,k)\n",
    "    C = cor(X)\n",
    "    n,p = size(X)\n",
    "    X_aug = augment_X(X)\n",
    "    M = 10^5\n",
    "    m = Model(with_optimizer(Gurobi.Optimizer, gurobi_env))\n",
    "    set_optimizer_attribute(m, \"OutputFlag\", 0)\n",
    "    set_optimizer_attribute(m, \"PSDTol\", 1)\n",
    "    @variable(m, beta[1:(p+1)])\n",
    "    @variable(m, z[1:p],Bin)\n",
    "    @variable(m, t[1:p])\n",
    "    @objective(m, Min, 1/2*sum((X_aug*beta.-y).^2)+gamma*sum(t[i] for i=1:p))\n",
    "    @constraint(m, [i=1:p], t[i]>= beta[i])\n",
    "    @constraint(m, [i=1:p], t[i]>= -beta[i])\n",
    "    @constraint(m, [i=1:p], beta[i]<= M*z[i])\n",
    "    @constraint(m, [i=1:p], -M*z[i]<=beta[i])\n",
    "    @constraint(m, sum(z)<=k)\n",
    "    #@constraint(m, [i=1:4:p-3], sum(z[i+j] for j=0:3)<=1)\n",
    "#     for i in 1:p\n",
    "#         for j in i+1:p\n",
    "#             if abs(C[i,j]) > rho\n",
    "#                 @constraint(m, z[i]+z[j] <= 1)\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "    optimize!(m)\n",
    "    return JuMP.value.(beta)\n",
    "end\n",
    "\n",
    "function fit_lasso(X, y)\n",
    "    cv = glmnetcv(X, y);\n",
    "    id_best = argmin(cv.meanloss);\n",
    "    betas = [GLMNet.coef(cv);cv.path.a0[id_best]];\n",
    "    return betas\n",
    "end\n",
    "\n",
    "function solve_inner_problem(X,Y,s,γ)\n",
    "    indices = findall(s .> 0.5)\n",
    "    n = length(Y)\n",
    "    denom = 2*n\n",
    "    Xs = X[:, indices]\n",
    "    α = Y - Xs * (inv(I / γ + Xs' * Xs) * (Xs'* Y))\n",
    "    obj = dot(Y, α) / denom\n",
    "    tmp = X' * α\n",
    "    grad = -γ .* tmp .^ 2 ./ denom\n",
    "  return obj, grad\n",
    "end\n",
    "\n",
    "function sparse_regression(X,Y,k,γ,s0=[],is_binary=false)\n",
    "    \n",
    "    m = Model(Gurobi.Optimizer)\n",
    "    set_optimizer_attribute(m, \"OutputFlag\", 0)\n",
    "    n,p = size(X)\n",
    "    \n",
    "    ###\n",
    "    # Step 1: Define the Variables:\n",
    "    ###\n",
    "    \n",
    "    if is_binary\n",
    "        @variable(m, s[1:p], Bin)\n",
    "        #@constraint(m, s[1:p] >= 0)\n",
    "    else\n",
    "        @variable(m, s[1:p]>=0)\n",
    "        @constraint(m, [i=1:p], s[i] <= 1)\n",
    "    end\n",
    "    \n",
    "    @variable(m, t >= 0)\n",
    "\n",
    "    ###\n",
    "    # Step 2: Set Up Constraints and Objective\n",
    "    ###\n",
    "    @constraint(m, sum(s) <= k)\n",
    "    # Initial solution: if none is provided, start at arbitrary point\n",
    "    if length(s0) == 0\n",
    "        s0 = zeros(p)\n",
    "        s0[1:k] .= 1\n",
    "    end\n",
    "    obj0, grad0 = solve_inner_problem(X,Y, s0, γ)\n",
    "    @constraint(m, t >= obj0 + dot(grad0, s - s0))\n",
    "    # Objective\n",
    "    @objective(m, Min, t)\n",
    "    \n",
    "    ###\n",
    "    # Step 3: Define the outer approximation function\n",
    "    ###\n",
    "    function outer_approximation(cb_data)\n",
    "        s_val = []\n",
    "        for i = 1:p\n",
    "            s_val = [s_val;callback_value(cb_data, s[i])]\n",
    "        end\n",
    "        obj, grad = solve_inner_problem(X,Y, s_val, γ)\n",
    "        # add the cut: t >= obj + sum(∇s * (s - s_val))\n",
    "        offset = sum(grad .* s_val)\n",
    "        con = @build_constraint(t >= obj + sum(grad[j] * s[j] for j=1:p) - offset)    \n",
    "        MOI.submit(m, MOI.LazyConstraint(cb_data), con)\n",
    "    end\n",
    "    MOI.set(m, MOI.LazyConstraintCallback(), outer_approximation)\n",
    "\n",
    "    ###\n",
    "    # Step 4: Solve\n",
    "    ###\n",
    "    optimize!(m)\n",
    "    s_opt = JuMP.value.(s)\n",
    "    \n",
    "    s_nonzeros = []\n",
    "    println(s_opt)\n",
    "    println(\"t: $(JuMP.value(t))\")\n",
    "    if !is_binary\n",
    "        idxes = sortperm(s_opt, rev=true)\n",
    "        s = zeros(p)\n",
    "        s[idxes[1:k]] = ones(k)\n",
    "        s_nonzeros = idxes\n",
    "    else\n",
    "        s_nonzeros = findall(x -> x>0.5, s_opt)\n",
    "    end\n",
    "    β = zeros(p)\n",
    "    X_s = X[:, s_nonzeros]\n",
    "    # Formula for the nonzero coefficients\n",
    "    β[s_nonzeros] = γ * X_s' * (Y - X_s * ((I / γ + X_s' * X_s) \\ (X_s'* Y)))\n",
    "    \n",
    "    #return Dict(\"support\" => s_opt, \"coefs\" => β, \"selected_features\" => s_nonzeros)\n",
    "    return is_binary ? [0;β] : β \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25b9d23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 271)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(CSV.File(df_path, header=1))\n",
    "df = df[shuffle(1:nrow(df))[1:100000], :]\n",
    "names(df)\n",
    "size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf9da1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = [\n",
    "    \"earnings_total\",\n",
    "    \"income_interest_dividends_rental\",\n",
    "    \"income_retirement\",\n",
    "    \"income_all\",\n",
    "    \"income_social_security\",\n",
    "    \"income_supplementary_security\",\n",
    "    \"income_total\",\n",
    "    \"income_self_employment\",\n",
    "    \"income_household\",\n",
    "    \"income_to_poverty_ratio\",\n",
    "    \"income_public_assistance\",\n",
    "    \"income_family\",\n",
    "    \"income_wages_salary\",\n",
    "    \"monthly_owner_costs\",\n",
    "    \"gross_rent\",\n",
    "    \"person_number\",\n",
    "    \"rent_monthly\",\n",
    "    \"property_value\"\n",
    "]\n",
    "cols = filter(x -> x ∉ excluded_cols, names(df))\n",
    "X, y = Matrix{Float32}(df[!, filter(x -> x != predictor_col, cols)]), df[!,predictor_col]\n",
    "\n",
    "X_train, y_train, X_test, y_test = partitionTrainTest(X, y, 0.7);\n",
    "X_train = normalize_data(X_train, normalization_type; is_train=true);\n",
    "X_test = normalize_data(X_test, normalization_type; is_train=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c28531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 253)"
     ]
    }
   ],
   "source": [
    "print(size(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9dfd5c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-09-05\n",
      "[1.0, -0.0, 1.0, 1.0, -0.0, 1.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 0.0, 0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, 1.0, 1.0, -0.0, 1.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, 1.0, 1.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, 1.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, 1.0, -0.0, 1.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 1.0, 1.0, -0.0, -0.0]\n",
      "t: 4.868003110097021e6\n",
      "r^2 train -8.269812628554263\n",
      "r^2 train -1.006581963519777\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "#betas, params = grid_search(X_train, y_train, solve_holistic_regr, calc_r2, \"Max\", 0.7; gamma=[0.1], rho=[0.7], k=[20])\n",
    "betas = fit_lasso(X_train, y_train)\n",
    "\n",
    "betas = sparse_regression(X_train, y_train, k ,1/sqrt(size(X_train,1)), 1.0*(betas[2:end] .>= 0.5), true)\n",
    "\n",
    "r2_c = calc_r2(X_test, y_test, betas)\n",
    "\n",
    "println(\"r^2 train $(calc_r2(X_train, y_train, betas))\")\n",
    "println(\"r^2 train $(calc_r2(X_test, y_test, betas))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = filter(x -> x ∉ excluded_cols, names(df))\n",
    "#important_features = cols[findall(abs.(betas_hol) .>= 0.01)]\n",
    "THRESHOLD = 0.000001\n",
    "println(\"Most important features:\")\n",
    "for i in sortperm(abs.(betas[2:end]), rev=true)\n",
    "    if abs(betas[i])<=THRESHOLD\n",
    "        continue\n",
    "    end\n",
    "    println(\"- $(cols[i]) : $(betas[i+1])\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "292017bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 253)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8eb5c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21.931198 seconds (71.57 k allocations: 34.170 GiB, 13.73% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fitted OptimalFeatureSelectionRegressor:\n",
       "  Constant: 59804.1\n",
       "  Weights:\n",
       "    x1:    9055.24\n",
       "    x109:  3629.04\n",
       "    x11:   5556.33\n",
       "    x110: -6391.03\n",
       "    x111:  6816.61\n",
       "    x112:  1331.61\n",
       "    x113:  3928.86\n",
       "    x114:  3389.5\n",
       "    x117: -2085.97\n",
       "    x118:  4314.06\n",
       "    x119: -3719.44\n",
       "    x12:   15128.3\n",
       "    x121:  4566.06\n",
       "    x124: -1787.86\n",
       "    x125: -1883.92\n",
       "    x126: -1513.73\n",
       "    x127:  1584.75\n",
       "    x128: -1306.04\n",
       "    x129: -1543.51\n",
       "    x130: -1856.4\n",
       "    x133: -1975.67\n",
       "    x134: -2568.94\n",
       "    x141: -2880.26\n",
       "    x144:  6975.3\n",
       "    x181:  415.65\n",
       "    x182: -415.65\n",
       "    x20:   6369.54\n",
       "    x205: -1752.56\n",
       "    x207: -1953.17\n",
       "    x21:   3925.91\n",
       "    x210:  1668.9\n",
       "    x220:  2010.13\n",
       "    x222: -1933.66\n",
       "    x223:  4475.71\n",
       "    x231:  2033.32\n",
       "    x238:  1411.55\n",
       "    x243:  2077.15\n",
       "    x3:    2316.59\n",
       "    x31:  -1500.16\n",
       "    x32:  -1339.27\n",
       "    x35:  -1105.74\n",
       "    x36:   3719.68\n",
       "    x4:    3150.41\n",
       "    x45:   2906.98\n",
       "    x50:  -8637.61\n",
       "    x6:   -9067.98\n",
       "    x64:  -1785.37\n",
       "    x7:   -5293.81\n",
       "    x77:   1671.02\n",
       "    x9:    15720.7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time begin\n",
    "    \n",
    "    m = IAI.OptimalFeatureSelectionRegressor(\n",
    "        sparsity=50\n",
    "    )\n",
    "\n",
    "    res = IAI.fit!(m, X_train, y_train)\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c059daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fitted OptimalFeatureSelectionRegressor:\n",
       "  Constant: 59804.1\n",
       "  Weights:\n",
       "    x1:    9055.24\n",
       "    x109:  3629.04\n",
       "    x11:   5556.33\n",
       "    x110: -6391.03\n",
       "    x111:  6816.61\n",
       "    x112:  1331.61\n",
       "    x113:  3928.86\n",
       "    x114:  3389.5\n",
       "    x117: -2085.97\n",
       "    x118:  4314.06\n",
       "    x119: -3719.44\n",
       "    x12:   15128.3\n",
       "    x121:  4566.06\n",
       "    x124: -1787.86\n",
       "    x125: -1883.92\n",
       "    x126: -1513.73\n",
       "    x127:  1584.75\n",
       "    x128: -1306.04\n",
       "    x129: -1543.51\n",
       "    x130: -1856.4\n",
       "    x133: -1975.67\n",
       "    x134: -2568.94\n",
       "    x141: -2880.26\n",
       "    x144:  6975.3\n",
       "    x181:  415.65\n",
       "    x182: -415.65\n",
       "    x20:   6369.54\n",
       "    x205: -1752.56\n",
       "    x207: -1953.17\n",
       "    x21:   3925.91\n",
       "    x210:  1668.9\n",
       "    x220:  2010.13\n",
       "    x222: -1933.66\n",
       "    x223:  4475.71\n",
       "    x231:  2033.32\n",
       "    x238:  1411.55\n",
       "    x243:  2077.15\n",
       "    x3:    2316.59\n",
       "    x31:  -1500.16\n",
       "    x32:  -1339.27\n",
       "    x35:  -1105.74\n",
       "    x36:   3719.68\n",
       "    x4:    3150.41\n",
       "    x45:   2906.98\n",
       "    x50:  -8637.61\n",
       "    x6:   -9067.98\n",
       "    x64:  -1785.37\n",
       "    x7:   -5293.81\n",
       "    x77:   1671.02\n",
       "    x9:    15720.7"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
